import pandas as pd
import pymysql
import logging
import json
from datetime import datetime
import boto3

# -------------------------#
# CONFIGURATION
# -------------------------#
ANALYTICS_TABLE = "time_analytics_model"
CHUNK_SIZE = 1000  # batch size for inserts

# -------------------------#
# LOGGER SETUP
# -------------------------#
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')

def log_msg(msg, level="info"):
    print(msg)
    if level == "info":
        logging.info(msg)
    elif level == "error":
        logging.error(msg)
    elif level == "warning":
        logging.warning(msg)

# -------------------------#
# LOAD DB CONFIG
# -------------------------#
def load_db_config():
    s3 = boto3.client('s3')
    bucket = 'timeanalytics'
    key = 'config/db_config.json'
    response = s3.get_object(Bucket=bucket, Key=key)
    return json.loads(response['Body'].read())

# -------------------------#
# ETL ENTRY POINT
# -------------------------#
def run_etl():
    start_time = datetime.utcnow()
    log_msg(f"ETL started at {start_time}")
    try:
        conn_config = load_db_config()
        df_raw = extract_task_data(conn_config)
        df_clean = transform_and_normalize(df_raw)
        load_to_reporting_table(df_clean, conn_config)
        validate_mappings_and_status(df_clean)
        log_pipeline_metrics(start_time, success=True)
    except Exception as e:
        log_pipeline_metrics(start_time, success=False, error=str(e))
        raise

# -------------------------#
# STEP 1: EXTRACT
# -------------------------#
def extract_task_data(config):
    connection = pymysql.connect(**config, cursorclass=pymysql.cursors.DictCursor)
    query = """
            -- Step 1: Base task metadata and delay label (only last 6 months)
            WITH task_core AS (
                SELECT
                    td.id AS task_id,
                    td.title AS task_title,
                    td.workflow_name,
                    td.worklfow_id,
                    td.customer_id,
                    td.task_bucket,
                    td.task_time_bucket,
                    td.status_name,
                    td.status_id,
                    td.due_date,
                    td.started_at,
                    td.completed_at,
                    td.completed_by,
                    td.created_by,
                    td.days_took,
                    td.users_assigned,
                    td.user_involved,
                    td.billable_time,
                    td.billable_amount,
                    td.task_revenue,
                    td.record_insert_at,
                    -- Derived Features
                    CASE
                        WHEN td.completed_at IS NOT NULL AND td.due_date IS NOT NULL THEN DATEDIFF(td.completed_at, td.due_date)
                        ELSE 0
                        END AS delay_days,
                    CASE
                        WHEN td.completed_at IS NOT NULL AND td.due_date IS NOT NULL AND td.completed_at > td.due_date THEN 1
                        ELSE 0
                        END AS is_delayed
                FROM task_details td
                WHERE td.is_active = TRUE
                  AND td.due_date >= CURDATE() - INTERVAL 6 MONTH
                AND td.started_at IS NOT NULL
                AND td.completed_at IS NOT NULL
                AND td.due_date IS NOT NULL
                ),

-- Step 2: Aggregated time metrics
                time_metrics AS (
            SELECT
                task_id,
                SUM(IFNULL(total_time, 0)) AS total_time_spent,
                SUM(IFNULL(non_billable_time, 0)) AS non_billable_time,
                SUM(IFNULL(billable_time, 0)) AS billable_time,
                SUM(IFNULL(billable_amount, 0)) AS total_billable_amount
            FROM time_model
            WHERE is_active = TRUE
            GROUP BY task_id
                ),

-- Step 3: Budget metrics
                budget_metrics AS (
            SELECT
                task_id,
                total_budget_hours,
                total_actual_hours,
                total_billable_amount,
                task_hours_variance
            FROM task_budget_hours
            WHERE is_active = TRUE
                ),

-- Step 4: Client metadata
                client_meta AS (
            SELECT
                serial_numer,
                client_id,
                client_type,
                industry_type,
                industry_type_id,
                realization_rate,
                production_rate
            FROM client_mapping
                )

-- Final Join
            SELECT
                tc.task_id,
                tc.task_title,
                tc.workflow_name,
                tc.worklfow_id,
                tc.task_bucket,
                tc.task_time_bucket,
                tc.status_name,
                tc.status_id,
                tc.due_date,
                tc.started_at,
                tc.completed_at,
                tc.completed_by,
                tc.created_by,
                tc.days_took,
                tc.users_assigned,
                tc.user_involved,
                tc.billable_time AS task_level_billable_time,
                tc.billable_amount AS task_level_billable_amount,
                tc.task_revenue,
                tc.delay_days,
                tc.is_delayed,

                tm.total_time_spent,
                tm.non_billable_time,
                tm.billable_time AS time_level_billable_time,
                tm.total_billable_amount AS time_level_billable_amount,

                bm.total_budget_hours,
                bm.total_actual_hours,
                bm.total_billable_amount AS budget_level_billable_amount,
                bm.task_hours_variance,

                cm.client_id,
                cm.client_type,
                cm.industry_type,
                cm.industry_type_id,
                cm.realization_rate,
                cm.production_rate

            FROM task_core tc
                     LEFT JOIN time_metrics tm ON tc.task_id = tm.task_id
                     LEFT JOIN budget_metrics bm ON tc.task_id = bm.task_id
                     LEFT JOIN client_meta cm ON tc.customer_id = cm.client_id; \
            """
    log_msg("Starting data extraction...")

    chunks = []
    total_rows = 0

    try:
        with connection.cursor() as cursor:
            cursor.execute(query)
            while True:
                rows = cursor.fetchmany(CHUNK_SIZE)
                if not rows:
                    break
                chunk_df = pd.DataFrame(rows)
                chunks.append(chunk_df)
                total_rows += len(chunk_df)
                log_msg(f"Fetched {total_rows} records so far...")

        log_msg(f"Total extracted rows: {total_rows}")
        return pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame()
    finally:
        connection.close()

# -------------------------#
# STEP 2: TRANSFORM
# -------------------------#
def transform_and_normalize(df):
    date_cols = ['due_date', 'started_at', 'completed_at', 'task_time_bucket']
    for col in date_cols:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')

    for col in ['status_name', 'client_type', 'industry_type']:
        if col in df.columns:
            df[col] = df[col].astype(str).str.lower().fillna("unknown")

    for col in ['task_title', 'workflow_name', 'task_bucket']:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip().str.replace(r'\s+', ' ', regex=True)

    for col in ['created_by', 'completed_by', 'users_assigned', 'user_involved']:
        if col in df.columns:
            df[col] = df[col].astype(str).str.lower().str.strip().replace('nan', '')

    numeric_cols = [
        'task_level_billable_time', 'task_level_billable_amount', 'task_revenue',
        'total_time_spent', 'non_billable_time', 'time_level_billable_time', 'time_level_billable_amount',
        'total_budget_hours', 'total_actual_hours', 'budget_level_billable_amount', 'task_hours_variance',
        'realization_rate', 'production_rate', 'days_took', 'delay_days', 'is_delayed'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)

    log_msg("Transform and normalize steps complete.")
    return df

# -------------------------#
# STEP 3: LOAD
# -------------------------#
def load_to_reporting_table(df, config):
    connection = pymysql.connect(**config)
    try:
        with connection.cursor() as cursor:
            cursor.execute(f"TRUNCATE TABLE {ANALYTICS_TABLE}")
            log_msg(f"Table {ANALYTICS_TABLE} truncated successfully.")

        for i in range(0, len(df), CHUNK_SIZE):
            chunk = df.iloc[i:i + CHUNK_SIZE]
            records = [tuple(x) for x in chunk.values]
            columns = ', '.join(chunk.columns)
            placeholders = ', '.join(['%s'] * len(chunk.columns))
            insert_query = f"INSERT INTO {ANALYTICS_TABLE} ({columns}) VALUES ({placeholders})"

            with connection.cursor() as cursor:
                cursor.executemany(insert_query, records)
                connection.commit()
                log_msg(f"Inserted {len(records)} rows into {ANALYTICS_TABLE}...")

    finally:
        connection.close()

# -------------------------#
# STEP 4: VALIDATION
# -------------------------#
def validate_mappings_and_status(df):
    log_msg(f"{df['client_id'].isnull().sum()} records missing client mapping.")
    log_msg(f"{df['status_name'].isnull().sum()} records missing status.")
    log_msg(f"{df['due_date'].isnull().sum() + df['completed_at'].isnull().sum()} records missing due_date or completed_at.")
    log_msg(f"{df['created_by'].isnull().sum()} records missing created_by.")
    log_msg(f"{df['completed_by'].isnull().sum()} records missing completed_by.")
    invalid_lifecycle = df[(df['started_at'] > df['completed_at']) | df['started_at'].isnull() | df['completed_at'].isnull()]
    if not invalid_lifecycle.empty:
        log_msg(f"{len(invalid_lifecycle)} tasks have invalid lifecycle timestamps.", level="warning")

# -------------------------#
# STEP 5: METRICS
# -------------------------#
def log_pipeline_metrics(start_time, success=True, error=None):
    end_time = datetime.utcnow()
    duration = (end_time - start_time).total_seconds()
    log_msg(f"ETL Status: {'SUCCESS' if success else 'FAILURE'}")
    log_msg(f"ETL Duration: {duration:.2f} seconds")
    if error:
        log_msg(f"Error encountered: {error}", level="error")
    if duration > 600:
        log_msg("ETL run took more than 10 minutes!", level="warning")

# -------------------------#
# RUN DAILY
# -------------------------#
if __name__ == "__main__":
    run_etl()
